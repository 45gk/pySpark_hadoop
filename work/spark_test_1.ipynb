{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "171842f8-62c3-4164-aac7-2b0beb011bdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "import os\n",
    "\n",
    "os.environ['HADOOP_USER_NAME'] = 'root'\n",
    "\n",
    "# 1. Создаем сессию Spark\n",
    "# Имя хоста 'namenode' берем из нашего docker-compose\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"QuickHDFSTest\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"dfs.client.use.datanode.hostname\", \"true\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"hdfs://namenode:9000/user/hive/warehouse\") \\\n",
    "    .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40325aa-e178-477b-8f31-d75f1dd18572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Тестовые данные\n",
    "data = [(\"Ivan\", 25), (\"Anna\", 30), (\"Petr\", 18), (\"Elena\", 45)]\n",
    "\n",
    "# 3. Описываем схему\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# 4. Создаем DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# 5. Показываем результат в консоли\n",
    "print(\"Наш легкий DataFrame:\")\n",
    "df.show()\n",
    "\n",
    "# 6. Записываем в HDFS (путь внутри контейнера к NameNode)\n",
    "# Мы создадим папку /user/spark_test\n",
    "try:\n",
    "    df.write.mode(\"overwrite\").parquet(\"hdfs://namenode:9000/user/spark_test\")\n",
    "    print(\"✅ Успешно сохранено в HDFS!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Ошибка записи: {e}\")\n",
    "\n",
    "# 7. Читаем обратно для проверки\n",
    "print(\"Чтение записанных данных из HDFS:\")\n",
    "df_load = spark.read.parquet(\"hdfs://namenode:9000/user/spark_test\")\n",
    "df_load.filter(df_load.age > 20).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da22f5f8-e26f-421e-8b2c-b78e65d9bfc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = [(14, \"Gold\"), (15, \"Silver\")]\n",
    "df = spark.createDataFrame(data, [\"client_id\", \"segment\"])\n",
    "\n",
    "# Сохраняем как таблицу (Spark создаст метаданные в папке spark-warehouse)\n",
    "df.write.mode(\"append\").saveAsTable(\"clients\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b1e67ee-8b74-4153-a61b-c5ea2908b440",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|client_id|  segment|\n",
      "+---------+---------+\n",
      "|        4|corporate|\n",
      "|       15|   Silver|\n",
      "|        2|   Silver|\n",
      "|        6|  premium|\n",
      "|        3|  premium|\n",
      "|        7|  premium|\n",
      "|        8|  premium|\n",
      "|        5|  premium|\n",
      "|        9|  premium|\n",
      "|       10|  premium|\n",
      "|       11|  premium|\n",
      "|       12|  premium|\n",
      "|       13|  premium|\n",
      "|       14|     Gold|\n",
      "|        1|     Gold|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Теперь SQL запрос будет работать!\n",
    "spark.sql(\"SELECT * FROM clients\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9621ad9a-7d8e-42cd-a8e7-425d44c528bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "CREATE TABLE default.clients (\n",
    "    client_id INTEGER,\n",
    "    segment STRING\n",
    ") USING PARQUET;\n",
    "''').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dac55768-13ae-4252-873b-70295e014577",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|client_id|  segment|\n",
      "+---------+---------+\n",
      "|        4|corporate|\n",
      "|        2|   Silver|\n",
      "|        3|  premium|\n",
      "|        5|  premium|\n",
      "|        1|     Gold|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "INSERT INTO default.clients (client_id, segment) VALUES\n",
    "--(1, 'retail'),\n",
    "--(2, 'retail'),\n",
    "(3, 'premium'),\n",
    "(4, 'corporate'),\n",
    "(5, 'premium')\n",
    "''')\n",
    "\n",
    "spark.sql(\"SELECT * FROM default.clients\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66468e44-ef52-4061-81e1-84053de44acf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "+--------------+---------+-------------------+------+\n",
      "|transaction_id|client_id|       operation_dt|amount|\n",
      "+--------------+---------+-------------------+------+\n",
      "|             6|        6|2026-01-05 10:10:00|  2700|\n",
      "|            14|        1|2026-02-01 08:00:00|   150|\n",
      "|             1|        1|2025-01-01 10:15:00|   100|\n",
      "|             7|        7|2026-01-05 10:10:00|  5700|\n",
      "|            15|        2|2026-02-02 09:30:00|   400|\n",
      "|             2|        1|2025-01-01 15:30:00|   -20|\n",
      "|             8|        8|2026-01-05 10:10:00|  7700|\n",
      "|             9|        9|2026-01-05 10:10:00|  8700|\n",
      "|            10|       10|2026-01-05 10:10:00|  2700|\n",
      "|            11|       11|2026-01-05 10:10:00|    70|\n",
      "|            12|       12|2026-01-05 10:10:00|     7|\n",
      "|            13|       13|2026-01-05 10:10:00|    27|\n",
      "|            16|        3|2025-12-31 10:10:00|   700|\n",
      "|             3|        2|2025-01-01 11:00:00|   300|\n",
      "|             4|        3|2025-01-01 09:45:00|   500|\n",
      "+--------------+---------+-------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "\n",
    "INSERT INTO transactions (transaction_id,client_id, operation_dt, amount) VALUES\n",
    "-- январь 2025\n",
    "-- март 2025\n",
    "(14, 1, '2026-02-01 08:00:00', 150.00),\n",
    "(15, 2, '2026-02-02 09:30:00', 400.00),\n",
    "(16, 3, '2025-12-31 10:10:00', 700.00);\n",
    "''').show()\n",
    "spark.sql(\"SELECT * FROM default.transactions\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef6a0ff7-c631-4aac-a786-9ce881b7590c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "+--------------+---------+-------------------+------+\n",
      "|transaction_id|client_id|       operation_dt|amount|\n",
      "+--------------+---------+-------------------+------+\n",
      "|             6|        6|2026-01-05 10:10:00|  2700|\n",
      "|            14|        1|2026-02-01 08:00:00|   150|\n",
      "|             1|        1|2025-01-01 10:15:00|   100|\n",
      "|            18|        2|2026-01-22 09:30:00|   400|\n",
      "|             7|        7|2026-01-05 10:10:00|  5700|\n",
      "|            15|        2|2026-02-02 09:30:00|   400|\n",
      "|             2|        1|2025-01-01 15:30:00|   -20|\n",
      "|             8|        8|2026-01-05 10:10:00|  7700|\n",
      "|             9|        9|2026-01-05 10:10:00|  8700|\n",
      "|            10|       10|2026-01-05 10:10:00|  2700|\n",
      "|            11|       11|2026-01-05 10:10:00|    70|\n",
      "|            12|       12|2026-01-05 10:10:00|     7|\n",
      "|            13|       13|2026-01-05 10:10:00|    27|\n",
      "|            17|        1|2026-01-31 08:00:00|   150|\n",
      "|            19|        3|2026-01-01 10:10:00|   700|\n",
      "|            16|        3|2025-12-31 10:10:00|   700|\n",
      "|             3|        2|2025-01-01 11:00:00|   300|\n",
      "|             4|        3|2025-01-01 09:45:00|   500|\n",
      "+--------------+---------+-------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "\n",
    "INSERT INTO transactions (transaction_id,client_id, operation_dt, amount) VALUES\n",
    "-- январь 2025\n",
    "-- март 2025\n",
    "(17, 1, '2026-01-31 08:00:00', 150.00),\n",
    "(18, 2, '2026-01-22 09:30:00', 400.00),\n",
    "(19, 3, '2026-01-01 10:10:00', 700.00);\n",
    "''').show()\n",
    "spark.sql(\"SELECT * FROM default.transactions\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4e8c61-19ab-4e77-9ac6-b5db199ad7a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "12505a72-55a4-49e2-abc2-c04d568d8cc2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-------------------+------+\n",
      "|transaction_id|client_id|       operation_dt|amount|\n",
      "+--------------+---------+-------------------+------+\n",
      "|             6|        6|2026-01-05 10:10:00|  2700|\n",
      "|             1|        1|2025-01-01 10:15:00|   100|\n",
      "|             7|        7|2026-01-05 10:10:00|  5700|\n",
      "|             2|        1|2025-01-01 15:30:00|   -20|\n",
      "|             8|        8|2026-01-05 10:10:00|  7700|\n",
      "|             9|        9|2026-01-05 10:10:00|  8700|\n",
      "|            10|       10|2026-01-05 10:10:00|  2700|\n",
      "|            11|       11|2026-01-05 10:10:00|    70|\n",
      "|            12|       12|2026-01-05 10:10:00|     7|\n",
      "|            13|       13|2026-01-05 10:10:00|    27|\n",
      "|             3|        2|2025-01-01 11:00:00|   300|\n",
      "|             4|        3|2025-01-01 09:45:00|   500|\n",
      "+--------------+---------+-------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM default.transactions\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f0386657-af15-4397-bca4-bf941ff74b0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-------------------+------+\n",
      "|transaction_id|client_id|       operation_dt|amount|\n",
      "+--------------+---------+-------------------+------+\n",
      "|             6|        6|2026-01-05 10:10:00|  2700|\n",
      "|            18|        2|2026-01-22 09:30:00|   400|\n",
      "|             7|        7|2026-01-05 10:10:00|  5700|\n",
      "|             8|        8|2026-01-05 10:10:00|  7700|\n",
      "|             9|        9|2026-01-05 10:10:00|  8700|\n",
      "|            10|       10|2026-01-05 10:10:00|  2700|\n",
      "|            11|       11|2026-01-05 10:10:00|    70|\n",
      "|            12|       12|2026-01-05 10:10:00|     7|\n",
      "|            13|       13|2026-01-05 10:10:00|    27|\n",
      "|            17|        1|2026-01-31 08:00:00|   150|\n",
      "|            19|        3|2026-01-01 10:10:00|   700|\n",
      "+--------------+---------+-------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "\n",
    "SELECT * FROM transactions\n",
    "WHERE operation_dt >= date_trunc('month', add_months(current_date(), -1))\n",
    "  AND operation_dt <  date_trunc('month', current_date())\n",
    "\n",
    "\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6b170388-5f1a-44c7-8ce1-316484e6cbd5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-------------------+------+\n",
      "|transaction_id|client_id|       operation_dt|amount|\n",
      "+--------------+---------+-------------------+------+\n",
      "|            14|        1|2026-02-01 08:00:00|   150|\n",
      "|            15|        2|2026-02-02 09:30:00|   400|\n",
      "+--------------+---------+-------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "\n",
    "SELECT * FROM transactions\n",
    "WHERE operation_dt <= date_trunc('month', add_months(current_date(), 1))\n",
    "  AND operation_dt >=  date_trunc('month', current_date())\n",
    "\n",
    "\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3108ab0b-a2f0-4394-a373-3d9f0f4a6f93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n",
      "|date_trunc(month, current_date())|\n",
      "+---------------------------------+\n",
      "|              2026-02-01 00:00:00|\n",
      "|              2026-02-01 00:00:00|\n",
      "|              2026-02-01 00:00:00|\n",
      "|              2026-02-01 00:00:00|\n",
      "|              2026-02-01 00:00:00|\n",
      "|              2026-02-01 00:00:00|\n",
      "|              2026-02-01 00:00:00|\n",
      "|              2026-02-01 00:00:00|\n",
      "|              2026-02-01 00:00:00|\n",
      "|              2026-02-01 00:00:00|\n",
      "|              2026-02-01 00:00:00|\n",
      "|              2026-02-01 00:00:00|\n",
      "|              2026-02-01 00:00:00|\n",
      "|              2026-02-01 00:00:00|\n",
      "|              2026-02-01 00:00:00|\n",
      "|              2026-02-01 00:00:00|\n",
      "|              2026-02-01 00:00:00|\n",
      "|              2026-02-01 00:00:00|\n",
      "+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "\n",
    "SELECT date_trunc('month', current_date()) FROM transactions\n",
    "\n",
    "\n",
    "\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bd3187-eeca-4594-860a-d3108b66464c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89bb4464-20d7-4c8a-a4a0-848ecde3fbe8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+------+\n",
      "|client_id|       operation_dt|amount|\n",
      "+---------+-------------------+------+\n",
      "|        6|2026-01-05 10:10:00|  2700|\n",
      "|        7|2026-01-05 10:10:00|  5700|\n",
      "|        8|2026-01-05 10:10:00|  7700|\n",
      "|        9|2026-01-05 10:10:00|  8700|\n",
      "|       10|2026-01-05 10:10:00|  2700|\n",
      "|       11|2026-01-05 10:10:00|    70|\n",
      "|       12|2026-01-05 10:10:00|     7|\n",
      "|       13|2026-01-05 10:10:00|    27|\n",
      "+---------+-------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "\n",
    "WITH last_month_tx AS (\n",
    "    SELECT\n",
    "        client_id,\n",
    "        operation_dt,\n",
    "        amount\n",
    "    FROM transactions\n",
    "    WHERE operation_dt >= date_trunc('month', add_months(current_date(), -1))\n",
    "      AND operation_dt <  date_trunc('month', current_date())\n",
    "),\n",
    "top_clients AS (\n",
    "    SELECT\n",
    "        client_id\n",
    "    FROM last_month_tx\n",
    "    GROUP BY client_id\n",
    "    ORDER BY SUM(amount) DESC\n",
    "    LIMIT 10\n",
    "),\n",
    "ranked_tx AS (\n",
    "    SELECT\n",
    "        t.client_id,\n",
    "        t.operation_dt,\n",
    "        t.amount,\n",
    "        ROW_NUMBER() OVER (\n",
    "            PARTITION BY t.client_id\n",
    "            ORDER BY t.operation_dt DESC\n",
    "        ) AS rn\n",
    "    FROM last_month_tx t\n",
    "    JOIN top_clients tc\n",
    "      ON t.client_id = tc.client_id\n",
    ")\n",
    "SELECT\n",
    "    client_id,\n",
    "    operation_dt,\n",
    "    amount\n",
    "FROM ranked_tx\n",
    "WHERE rn <= 3;\n",
    "\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96c67257-4984-414a-9d78-a4ba455a6d12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "CREATE TABLE daily_client_balance (\n",
    "    client_id INTEGER,\n",
    "    balance_date STRING,\n",
    "    balance_amount DECIMAL,\n",
    "    load_dt STRING\n",
    ") USING PARQUET;\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0ae6eaa-656b-4aac-8f61-0d52befbc4b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "INSERT INTO daily_client_balance (client_id, balance_date, balance_amount, load_dt) VALUES\n",
    "(1, '2025-01-01', 80.00, '2025-01-02'),\n",
    "(2, '2025-01-01', 300.00, '2025-01-02'),\n",
    "(3, '2025-01-01', 500.00, '2025-01-02');\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e5bc5ee-55c1-4258-aad5-fc5af9663486",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+----------------+----+\n",
      "|balance_date|balance_sum|transactions_sum|diff|\n",
      "+------------+-----------+----------------+----+\n",
      "|  2025-01-01|        160|              80|  80|\n",
      "|  2025-01-01|        300|             300|   0|\n",
      "|  2025-01-01|        500|             500|   0|\n",
      "+------------+-----------+----------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT\n",
    "    d.balance_date,\n",
    "    SUM(d.balance_amount)      AS balance_sum,\n",
    "    SUM(t.amount)              AS transactions_sum,\n",
    "    SUM(d.balance_amount) - SUM(t.amount) AS diff\n",
    "FROM daily_client_balance d\n",
    "JOIN transactions t\n",
    "  ON d.client_id = t.client_id\n",
    " AND DATE(t.operation_dt) = DATE(d.balance_date)\n",
    "-- WHERE DATE(d.balance_date) = '2025-01-01'\n",
    "GROUP BY d.client_id, d.balance_date;\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdadefd-6dc3-480b-b4a8-ed5327706ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
